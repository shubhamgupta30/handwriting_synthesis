{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import drawing\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import torch \n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "\n",
    "n_output_mixtures = 20\n",
    "n_attn_mixtures = 1\n",
    "hidden_size = 400\n",
    "input_size = 3\n",
    "batch_size = 128\n",
    "n_epochs = 1000\n",
    "timestamps = 1200\n",
    "lr = 1e-5\n",
    "eps = 1e-6\n",
    "model_name = \"20210811_Synthesis-K1\"\n",
    "base_dir = \"/u/home/lyrebird_code/handwriting_synthesis/model_checkpoints/\"\n",
    "embedding_size = None # To be set to 73 later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2,1,  3)\n",
    "a\n",
    "nn.Softmax(0)(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45785b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "data = [np.load(os.path.join(data_dir, '{}.npy'.format(i))) for i in ['x', 'x_len', 'c', 'c_len', 'w_id']]\n",
    "strokes_og = data[0]\n",
    "stroke_lens = data[1]\n",
    "strings = data[2]\n",
    "string_lens = data[3]\n",
    "w_id = data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52880d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "alphabet = [\n",
    "    '\\x00', ' ', '!', '\"', '#', \"'\", '(', ')', ',', '-', '.',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';',\n",
    "    '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K',\n",
    "    'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "    'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
    "    'y', 'z'\n",
    "]\n",
    "vocab_size = len(alphabet)\n",
    "eos_char = '\\x00'\n",
    "alpha_to_num = defaultdict(int, list(map(reversed, enumerate(alphabet))))\n",
    "num_to_alpha = defaultdict(int, list(enumerate(alphabet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa588523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize(strokes, lengths):\n",
    "    points_to_consider = np.zeros((lengths.sum() - 2*len(lengths), 2))\n",
    "    idx = 0\n",
    "    for i in range(0, len(lengths)):\n",
    "        #print(strokes[i][1:lengths[i], 0:2].shape)\n",
    "        points_to_consider[idx:idx+lengths[i]-2] = strokes[i][1:lengths[i]-1, 0:2]\n",
    "        idx += lengths[i] - 2\n",
    "    means = points_to_consider.mean(axis=0)\n",
    "    std = points_to_consider.std(axis=0)\n",
    "    \n",
    "    result = strokes.copy()\n",
    "    for i in range(len(lengths)):\n",
    "        result[i, 1:lengths[i]-1, 0:2] -= means\n",
    "        result[i, 1:lengths[i]-1, 0:2] /= std\n",
    "    return result, means, std\n",
    "\n",
    "def destandarize(strokes_standarized, lengths, means, std):\n",
    "    result = strokes_standarized.copy()\n",
    "    for i in range(len(lengths)):\n",
    "        result[i, 1:lengths[i]-1, 0:2] *= std\n",
    "        result[i, 1:lengths[i]-1, 0:2] += means\n",
    "    return result\n",
    "\n",
    "def preprocess(strokes_og, stroke_lens, strings, string_lengths):\n",
    "    # My preprocesisng is simple - all strokes start with (0, 0, 0) nd end with (0, 0, 1)\n",
    "\n",
    "    # to this end we remove all full length strokes to make space for the end \n",
    "    # (0, 0, 1) as the end point\n",
    "\n",
    "    indices = stroke_lens != 1200\n",
    "    strokes_og = strokes_og[indices]\n",
    "    stroke_lens = stroke_lens[indices]\n",
    "    strings = strings[indices]\n",
    "    string_lengths = string_lengths[indices]\n",
    "\n",
    "    # Now we make sure that every stroke has the correct \n",
    "    # start point of (0, 0, 0) and end point of (0, 0, 1)\n",
    "    strokes_og[:,0,2] = 0\n",
    "    for i in range(strokes_og.shape[0]):\n",
    "        strokes_og[i, stroke_lens[i], 2] = 1\n",
    "    stroke_lens += 1\n",
    "    \n",
    "    # Standarize the coordinates separaetly to have zero mean and std dev 1\n",
    "    standarized_strokes, means, std = standarize(strokes_og, stroke_lens)\n",
    "    \n",
    "    texts = [drawing.decode_ascii(s) + eos_char for s in strings]\n",
    "    text_lengths = [len(t) for t in texts]\n",
    "    \n",
    "    max_text_length = max(text_lengths)\n",
    "    num_texts = len(texts)\n",
    "    one_hots = torch.zeros(num_texts, max_text_length, vocab_size, device=device, dtype=torch.float64)\n",
    "    for i, text in enumerate(texts):\n",
    "        for j, c in enumerate(text):\n",
    "            one_hots[i, j, alpha_to_num[c]] = 1\n",
    "    \n",
    "    return standarized_strokes, means, std, stroke_lens, texts, text_lengths, one_hots, vocab_size\n",
    "\n",
    "def string_to_one_hot(string):\n",
    "    if (string[-1] != eos_char):\n",
    "        string += eos_char\n",
    "    one_hot = torch.zeros(len(string), vocab_size, device=device)\n",
    "    for i, c in enumerate(string):\n",
    "        one_hot[i, alpha_to_num[c]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def one_hot_to_string(one_hot):\n",
    "    # assuming that one_hot is numpy\n",
    "    def get_char(arr):\n",
    "        arr = list(arr)\n",
    "        if 1 not in arr:\n",
    "            return ''\n",
    "        return num_to_alpha[list(arr).index(1)]\n",
    "    return ''.join([get_char(a) for a in one_hot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dae4b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_hot_to_string(string_to_one_hot(\"sample\").cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a962aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "strokes, means, std, stroke_lengths, texts, text_lengths, one_hots, embedding_size  = preprocess(\n",
    "    strokes_og, stroke_lens, strings, string_lens)\n",
    "#print(strokes.shape, stroke_lengths.shape, means.shape, std.shape, texts.shape, text_lengths.shape, one_hots.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988387d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strokes[2][stroke_lengths[2]-2:stroke_lengths[2]+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randint(low=0, high=len(texts), size=(1,)).item()\n",
    "print(idx)\n",
    "texts[idx]\n",
    "text_lengths[idx]\n",
    "one_hot_to_string(one_hots[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da8c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def offsets_to_coords(offsets):\n",
    "    \"\"\"\n",
    "    convert from offsets to coordinates\n",
    "    \"\"\"\n",
    "    return np.concatenate([np.cumsum(offsets[:, :2], axis=0), offsets[:, 2:3]], axis=1)\n",
    "\n",
    "\n",
    "def draw(offsets, plot_end_points=True):\n",
    "    strokes = offsets_to_coords(offsets)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    stroke = []\n",
    "    for x, y, eos in strokes:\n",
    "        stroke.append((x, y))\n",
    "        if eos == 1:\n",
    "            coords = list(zip(*stroke))\n",
    "            ax.plot(coords[0], coords[1], 'k')\n",
    "            if plot_end_points:\n",
    "                ax.plot(x, y, 'ro')\n",
    "            stroke = []\n",
    "    if stroke:\n",
    "        coords = list(zip(*stroke))\n",
    "        ax.plot(coords[0], coords[1], 'k')\n",
    "        stroke = []\n",
    "\n",
    "    padding = 10\n",
    "    \n",
    "    ax.set_xlim(strokes[:,0].min() - padding, strokes[:,0].max() + padding)\n",
    "    ax.set_ylim(strokes[:,1].min() - padding, strokes[:,1].max() + padding)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tick_params(\n",
    "        axis='both',\n",
    "        left='off',\n",
    "        top='off',\n",
    "        right='off',\n",
    "        bottom='off',\n",
    "        labelleft='off',\n",
    "        labeltop='off',\n",
    "        labelright='off',\n",
    "        labelbottom='off'\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "    \n",
    "\n",
    "def attention_plot(phis):\n",
    "    phis = phis.cpu().numpy()\n",
    "    _=plt.plot(phis.T)\n",
    "    plt.show()\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "    #phis= phis/(phis.sum(dim = 0) + eps)\n",
    "    plt.xlabel('handwriting generation')\n",
    "    plt.ylabel('text scanning')\n",
    "    plt.imshow(phis, cmap='Greys', interpolation='nearest', aspect='auto')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8dd26b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw(strokes[0])\n",
    "res2 = destandarize(strokes, stroke_lengths, means, std) # for checking\n",
    "draw(res2[0])\n",
    "print(drawing.decode_ascii(strings[0]))\n",
    "\n",
    "a = torch.randint(low = 0, high = 10, size = (5, 20))\n",
    "a\n",
    "attention_plot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5fe9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, strokes, stroke_lengths, one_hots, text_lengths):\n",
    "        assert len(strokes) == len(one_hots)\n",
    "        self.strokes = strokes\n",
    "        self.stroke_lengths = stroke_lengths\n",
    "        self.one_hots = one_hots\n",
    "        self.one_hot_lengths = text_lengths\n",
    "        self.stroke_mask = self.getStrokeMask()\n",
    "        self.one_hot_mask = self.getOneHotMask()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.strokes.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.strokes[idx], self.stroke_lengths[idx], self.stroke_mask[idx], self.one_hots[idx], self.one_hot_lengths[idx], self.one_hot_mask[idx]\n",
    "    \n",
    "    def getStrokeMask(self):\n",
    "        mask = np.ones((self.strokes.shape[0], self.strokes.shape[1]))\n",
    "        for i in range(self.strokes.shape[0]):\n",
    "            # The mask is true to the string lengths.\n",
    "            # Any offsets for Special training of RNN needs to be handled separately\n",
    "            mask[i][self.stroke_lengths[i]:] = 0\n",
    "        return mask\n",
    "    \n",
    "    def getOneHotMask(self):\n",
    "        mask = np.ones((self.one_hots.shape[0], self.one_hots.shape[1]))\n",
    "        for i in range(self.one_hots.shape[0]):\n",
    "            # The mask is true to the string lengths.\n",
    "            # Any offsets for Special training of RNN needs to be handled separately\n",
    "            mask[i][self.one_hot_lengths[i]:] = 0\n",
    "        return mask\n",
    "    \n",
    "train_dataset = HandwritingDataset(strokes[:-150], stroke_lengths[:-150], one_hots[:-150], text_lengths[:-150])\n",
    "test_dataset = HandwritingDataset(strokes[-150:], stroke_lengths[-150:], one_hots[-150:], text_lengths[-150:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e991a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dl = DataLoader(\n",
    "#     train_dataset,\n",
    "#     shuffle=True,\n",
    "#     batch_size=2,\n",
    "#     drop_last=True)\n",
    "\n",
    "# k = iter(dl)\n",
    "# for j in range(1):\n",
    "#     print(\"JJJJJJJJJJJJJJJJJ:\", j)\n",
    "#     s, l, m, oh, ohl, ohm = next(k)\n",
    "#     l0 = l[0]\n",
    "#     l1 = l[1]\n",
    "#     l\n",
    "#     s.shape\n",
    "#     m.shape\n",
    "\n",
    "#     print(\"Testing the 0th position of every stroke\")\n",
    "#     s[:,0,:]\n",
    "\n",
    "#     print(\"testing the length location of stroke\")\n",
    "#     s[0,l0-2:l0+2, :]\n",
    "#     s[1,l1-2:l1+2, :]\n",
    "\n",
    "#     print(\"testing the length location of mask\")\n",
    "#     m[0,l0-2:l0+2]\n",
    "#     m[1,l1-2:l1+2]\n",
    "    \n",
    "    \n",
    "#     l0 = ohl[0]\n",
    "#     l1 = ohl[1]\n",
    "#     ohl\n",
    "#     oh.shape\n",
    "#     ohm.shape\n",
    "\n",
    "#     print(\"Testing the 0th position of every one hot\")\n",
    "#     oh[:,0,:]\n",
    "\n",
    "#     print(\"testing the length location of one hot\")\n",
    "#     oh[0,l0-2:l0+2, :]\n",
    "#     oh[1,l1-2:l1+2, :]\n",
    "\n",
    "#     print(\"testing the length location of mask\")\n",
    "#     ohm[0,l0-2:l0+2]\n",
    "#     ohm[1,l1-2:l1+2]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75add5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs = 2\n",
    "# u = torch.arange(1, 11, device=device)[None, :, None].repeat(bs, 1, 1)\n",
    "# alpha = torch.rand(bs, 5)[:, None]\n",
    "# beta = torch.rand(bs, 5)[:, None]\n",
    "# kappa = torch.randn(bs, 5)[:, None]\n",
    "# u, u.shape\n",
    "# alpha, alpha.shape\n",
    "# beta, beta.shape\n",
    "# kappa, kappa.shape\n",
    "# res = alpha * torch.exp(-beta * (kappa - u)**2)\n",
    "# res.shape\n",
    "# res.sum(dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2302dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = torch.rand(2, 9)\n",
    "# b[None].shape\n",
    "# b[:, None].shape\n",
    "# b[:, :, None].shape\n",
    "# b.chunk(5, -1)\n",
    "\n",
    "# a = torch.rand(2, 3, 4)\n",
    "# wt = torch.randn(2, 3)[:, :, None]\n",
    "# a \n",
    "# wt\n",
    "# (a * wt).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianAttention(nn.Module):\n",
    "    def __init__(self, lstm_hidden_size, n_mixtures):\n",
    "        super(GaussianAttention, self).__init__()\n",
    "        self.n_mixtures = n_mixtures\n",
    "        self.linear_layer = nn.Linear(lstm_hidden_size, 3*n_mixtures)\n",
    "        \n",
    "    def forward(self, h, kappa_prev, one_hot_batch, one_hot_mask, eps=1e-6):\n",
    "        B, T = one_hot_mask.shape\n",
    "        \n",
    "        out_1 = self.linear_layer(h) # (B, 3*K)\n",
    "        alpha, beta, kappa = (torch.exp(out_1) + eps)[:, None].chunk(3, dim=-1) # (B, 1, K) each\n",
    "        #kappa = (kappa + kappa_prev) * 0.3\n",
    "        kappa = kappa * 0.04 + kappa_prev\n",
    "        \n",
    "        u = torch.arange(0, T, device=\"cpu\")[None, :, None].repeat(B, 1, 1).to(device) # (B, T, 1)\n",
    "        \n",
    "        phi = alpha * torch.exp(-beta * torch.pow(kappa - u, 2)) # (B, T, K)\n",
    "        phi = phi.sum(dim=-1) # (B, T)\n",
    "        \n",
    "        phi = (phi * one_hot_mask)[:, :, None] # (B, T, 1)\n",
    "        \n",
    "        w = (phi * one_hot_batch).sum(1) # (B, V) V = vocab_size\n",
    "        \n",
    "        attn_params = {\n",
    "            \"w\": w,\n",
    "            \"phi\": phi,\n",
    "            \"alpha\": alpha,\n",
    "            \"beta\": beta,\n",
    "            \"kappa\": kappa,\n",
    "            \"out_1\": out_1\n",
    "        }\n",
    "\n",
    "        return attn_params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff911da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 2\n",
    "# hidden_size = 4\n",
    "# input_size = 3\n",
    "# n_mixtures = 5\n",
    "# vocab_size = 6\n",
    "# timesteps = 7\n",
    "\n",
    "# h = torch.randn(batch_size, hidden_size)\n",
    "# kappa_prev = torch.randn(batch_size, n_mixtures)[:,None,:]\n",
    "# ohb = torch.randn(batch_size, timesteps, vocab_size)\n",
    "# ohm = torch.ones(batch_size, timesteps)\n",
    "# ohm[0, 5:] = 0\n",
    "# ohm[1, 3:] = 0\n",
    "# ohm\n",
    "# gl = GaussianAttention(hidden_size, n_mixtures)\n",
    "# gl(h, kappa_prev, ohb, ohm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange \n",
    "\n",
    "class HandwritingSynthesis(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_output_mixtures, n_attn_mixtures, embedding_size):\n",
    "        super(HandwritingSynthesis, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_output_mixtures = n_output_mixtures\n",
    "        self.n_attn_mixtures = n_attn_mixtures\n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_size = 6*n_output_mixtures + 1\n",
    "        \n",
    "        self.lstm1 = nn.LSTMCell(input_size + embedding_size, hidden_size)\n",
    "        self.gaussian_attn = GaussianAttention(hidden_size, n_attn_mixtures)\n",
    "        self.lstm2 = nn.LSTMCell(embedding_size + input_size + hidden_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, stroke_t, one_hots, one_hot_mask, prev_state):\n",
    "        bs = stroke_t.shape[0]\n",
    "        # K: n_attn_mixtures, E: embedding_size, H: hidden_size, T: Total timesteps\n",
    "        # stroke_t: (B,3), kappa: (B,1,K), attn: (B,E), hidden h and c : (B, H)\n",
    "        # one_hots : (B, T, E), one_hot_mask = (B, T)\n",
    "        attn_prev, kappa_prev, hid_1, hid_2 = prev_state[\"w\"], prev_state[\"kappa\"], prev_state[\"hidden_1\"], prev_state[\"hidden_2\"]\n",
    "        \n",
    "        hid_1 = self.lstm1(\n",
    "            torch.cat([stroke_t, attn_prev], dim=-1),\n",
    "            hid_1)\n",
    "        \n",
    "        params_new = self.gaussian_attn(\n",
    "            hid_1[0],\n",
    "            kappa_prev,\n",
    "            one_hots,\n",
    "            one_hot_mask)\n",
    "        \n",
    "        attn_new = params_new[\"w\"]\n",
    "        \n",
    "        hid_2 = self.lstm2(\n",
    "            torch.cat([stroke_t, attn_new, hid_1[0]], dim=-1),\n",
    "            hid_2)\n",
    "        \n",
    "        out = self.linear(hid_2[0])\n",
    "        \n",
    "        params_new[\"hidden_1\"] = hid_1\n",
    "        params_new[\"hidden_2\"] = hid_2\n",
    "        \n",
    "        return out, params_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d7d7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# batch_size = 2\n",
    "# input_size = 3\n",
    "# hidden_size = 4\n",
    "# n_output_mixtures = 8\n",
    "# n_attn_mixtures = 5\n",
    "# embedding_size = 6\n",
    "# timestamps = 7\n",
    "\n",
    "# h = HandwritingSynthesis(input_size, hidden_size, n_output_mixtures, n_attn_mixtures, embedding_size)\n",
    "\n",
    "# stroke = torch.randn(batch_size, 3)\n",
    "# stroke[:, 2] = 0\n",
    "# stroke\n",
    "\n",
    "# one_hots = torch.zeros(batch_size, timestamps, embedding_size)\n",
    "# one_hots[0,:5, 0]= 1\n",
    "# one_hots[1, :3, 3] =1\n",
    "# print(\"one_hots\", one_hots)\n",
    "\n",
    "# one_hot_mask = torch.ones(batch_size, timestamps)\n",
    "# one_hot_mask[0,5:] = 0\n",
    "# one_hot_mask[1,3:] = 0\n",
    "# print(\"one_hot_mask\", one_hot_mask)\n",
    "\n",
    "# attn = {\n",
    "#     \"w\": torch.randn(batch_size, embedding_size),\n",
    "#     \"phi\": torch.randn(batch_size, timestamps, 1),\n",
    "#     \"alpha\": torch.randn(batch_size, 1, n_attn_mixtures),\n",
    "#     \"beta\": torch.randn(batch_size, 1, n_attn_mixtures),\n",
    "#     \"kappa\": torch.randn(batch_size, 1, n_attn_mixtures)\n",
    "# }\n",
    "# prev_state = (attn,\n",
    "#               (torch.zeros(batch_size, hidden_size), torch.zeros(batch_size, hidden_size)),\n",
    "#               (torch.zeros(batch_size, hidden_size), torch.zeros(batch_size, hidden_size)))\n",
    "# print(\"prev_state\", prev_state)\n",
    "\n",
    "\n",
    "# h(stroke, one_hots, one_hot_mask, prev_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3024237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mixture_params_from_output(outputs, bias=0):\n",
    "    # outputs:  b, max_len-1, 6*n_output_mixtures+1\n",
    "    pis = nn.Softmax(2)((1+bias)*outputs[:,:,:n_output_mixtures])\n",
    "    mus = rearrange(outputs[:,:,n_output_mixtures:3*n_output_mixtures], 'b l (n d) -> b l n d', d=2)\n",
    "    \n",
    "    sigmas = rearrange(torch.exp(outputs[:,:,3*n_output_mixtures:5*n_output_mixtures]-bias), 'b l (n d) -> b l n d', d=2) + eps\n",
    "    phos = rearrange((1-eps) * torch.tanh(outputs[:,:,5*n_output_mixtures:6*n_output_mixtures]), 'b l (n d) -> b l n d', d=1)\n",
    "\n",
    "    \n",
    "    covs = torch.zeros(outputs.shape[0], outputs.shape[1], n_output_mixtures, 2, 2, device=device)\n",
    "    covs[:,:,:,0,0] = sigmas[:,:,:,0]**2\n",
    "    covs[:,:,:,1,1] = sigmas[:,:,:,1]**2\n",
    "    covs[:,:,:,0,1] = phos[:,:,:,0] * sigmas[:,:,:,0] * sigmas[:,:,:,1]\n",
    "    covs[:,:,:,1,0] = covs[:,:,:,0,1]\n",
    "    \n",
    "    return pis, mus, covs\n",
    "\n",
    "def get_mixture_distributions_from_output(outputs):\n",
    "    pis, mus, covs = get_mixture_params_from_output(outputs)\n",
    "    \n",
    "    distributions = torch.distributions.MultivariateNormal(mus, covs)\n",
    "    return pis, distributions\n",
    "\n",
    "def get_pen_lift_probs_from_output(outputs):\n",
    "    return 1/(1 + torch.exp(outputs[:,:,-1]))\n",
    "\n",
    "def nll(outputs, targets, mask_batch):\n",
    "    # outputs:  b, max_len-1, 6*n_mixtures+1\n",
    "    # targets: b, max_len-1, 3\n",
    "    outputs = outputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    target_coords = targets[:,:,0:2].unsqueeze(2).repeat_interleave(\n",
    "            torch.tensor([n_output_mixtures], device=device), dim=2)\n",
    "    # target_coords: b, max_len-1, n_mixtures, 3\n",
    "    stroke_lift = targets[:,:,-1] # b, max_len-1, 1\n",
    "    \n",
    "    pis, distributions = get_mixture_distributions_from_output(outputs)\n",
    "    es = get_pen_lift_probs_from_output(outputs)\n",
    "\n",
    "    probs = distributions.log_prob(target_coords) + eps\n",
    "    loss1 = - torch.logsumexp(torch.log(pis) + probs, dim=2)\n",
    "    loss2 = - torch.log(es)*stroke_lift\n",
    "    loss3 = - torch.log(1 - es)*(1-stroke_lift)\n",
    "    loss_per_point = loss1 + loss2 + loss3\n",
    "    loss_per_point *= mask_batch\n",
    "    \n",
    "    return loss_per_point.sum()/outputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86930d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_prev_states(mode='train'):\n",
    "    dim = batch_size if mode == 'train' else 1\n",
    "    h01 = torch.zeros(dim, hidden_size, device=device)\n",
    "    c01 = torch.zeros(dim, hidden_size, device=device)\n",
    "    h02 = torch.zeros(dim, hidden_size, device=device)\n",
    "    c02 = torch.zeros(dim, hidden_size, device=device)\n",
    "    kappa = torch.zeros(dim, 1, n_attn_mixtures, device=device)\n",
    "    w = torch.zeros(dim, embedding_size, device=device)\n",
    "    \n",
    "    prev_states = {\n",
    "        \"kappa\": torch.zeros(dim, 1, n_attn_mixtures, device=device),\n",
    "        \"w\": torch.zeros(dim, embedding_size, device=device),\n",
    "        \"hidden_1\": (h01, c01),\n",
    "        \"hidden_2\": (h02, c02)\n",
    "    }\n",
    "    return prev_states\n",
    "\n",
    "def get_next_point(model, point_prev, text_one_hot, mask, prev_states, bias=0):\n",
    "    with torch.no_grad():\n",
    "        outputs, prev_states = model(point_prev.unsqueeze(0), text_one_hot, mask, prev_states)\n",
    "        \n",
    "        outputs = outputs[:, None]\n",
    "        \n",
    "        es = get_pen_lift_probs_from_output(outputs)\n",
    "        pis, mus, covs = get_mixture_params_from_output(outputs, bias=0)\n",
    "        \n",
    "        sample_index = 0\n",
    "        if n_output_mixtures > 1:\n",
    "            sample_index = np.random.choice(\n",
    "                range(n_output_mixtures),\n",
    "                p = pis.squeeze().cpu().numpy())\n",
    "\n",
    "        pen_off = torch.tensor(\n",
    "            [np.random.binomial(1,es.item())],\n",
    "            device=device)\n",
    "\n",
    "        sample_point = torch.tensor(\n",
    "            np.random.multivariate_normal(\n",
    "                mus.squeeze(0).squeeze(0)[sample_index].cpu().numpy(),\n",
    "                covs.squeeze(0).squeeze(0)[sample_index].cpu().numpy()),\n",
    "            device=device)\n",
    "        \n",
    "        return torch.cat((sample_point, pen_off), dim=0), prev_states\n",
    "        \n",
    "def sample(model, text, bias=0):\n",
    "    timestamps = 500\n",
    "    text_one_hot = string_to_one_hot(text)[None]\n",
    "    print(\"text_one_hot\", text_one_hot.shape, text_one_hot)\n",
    "    mask = torch.ones(1, text_one_hot.shape[1], device=device)\n",
    "    print(\"mask\", mask.shape, mask)\n",
    "    sample_stroke = torch.zeros(timestamps, 3, device=device)\n",
    "    prev_states = get_initial_prev_states(\"sample\")\n",
    "\n",
    "    phis = []\n",
    "    hid1s = []\n",
    "    hid2s = []\n",
    "    alphas = []\n",
    "    betas = []\n",
    "    kappas = []\n",
    "    for i in range(timestamps-2):\n",
    "        prev_point = sample_stroke[i]\n",
    "        new_point, prev_states = get_next_point(model, prev_point, text_one_hot, mask, prev_states, bias=0)\n",
    "        phis.append(prev_states[\"phi\"].squeeze()[None])\n",
    "        hid1s.append(prev_states[\"hidden_1\"][0].squeeze()[None])\n",
    "        hid2s.append(prev_states[\"hidden_2\"][0].squeeze()[None])\n",
    "        alphas.append(prev_states[\"alpha\"].squeeze()[None])\n",
    "        betas.append(prev_states[\"beta\"].squeeze()[None])\n",
    "        kappas.append(prev_states[\"kappa\"].squeeze()[None])\n",
    "        sample_stroke[i+1] = new_point\n",
    "    phis = torch.cat(phis, dim=0).T\n",
    "    hid1s = torch.cat(hid1s, dim=0).T\n",
    "    hid2s = torch.cat(hid2s, dim=0).T\n",
    "    alphas = torch.cat(alphas, dim=0).T\n",
    "    betas = torch.cat(betas, dim=0).T\n",
    "    kappas = torch.cat(kappas, dim=0).T\n",
    "    return sample_stroke, phis, hid1s, hid2s, alphas, betas, kappas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "def get_inputs_targets_mask(strokes_batch, strokes_mask):\n",
    "    inputs = strokes_batch[:, :-1, :]\n",
    "    targets = strokes_batch[:, 1:, :]\n",
    "    mask = strokes_mask[:, 1:]\n",
    "    return inputs, targets, mask\n",
    "\n",
    "def train_batch(model,\n",
    "                optimizer,\n",
    "                strokes,\n",
    "                strokes_mask,\n",
    "                one_hots,\n",
    "                one_hot_lengths,\n",
    "                one_hots_mask,\n",
    "                prev_states,\n",
    "                output_holder):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    inputs, targets, mask = get_inputs_targets_mask(strokes, strokes_mask)\n",
    "    \n",
    "    T = inputs.shape[1]\n",
    "    \n",
    "    for t in range(T): \n",
    "        output_holder[t], prev_states = model(\n",
    "            inputs[:, t, :],\n",
    "            one_hots,\n",
    "            one_hots_mask,\n",
    "            prev_states)\n",
    "    loss = nll(rearrange(output_holder, 'T B S -> B T S'), targets, mask)\n",
    "    loss.backward()\n",
    "    \n",
    "    #### Do Gradient clipping here if need be\n",
    "    torch.nn.utils.clip_grad_value_(model.parameters(), 10)\n",
    "    torch.nn.utils.clip_grad_value_(loss, 100)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), prev_states[\"kappa\"].squeeze()\n",
    "\n",
    "def save_model(model, model_name):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }, base_dir + model_name)\n",
    "    \n",
    "def detach(x):\n",
    "    if type(x) is tuple:\n",
    "        return (x[0], x[1])\n",
    "    if type(x) is dict:\n",
    "        d = {}\n",
    "        for k in x:\n",
    "            d[k] = x[k].detach()\n",
    "        return d\n",
    "    return x.detach()\n",
    "    \n",
    "def train(model, optimizer, train_dataloader, num_epochs):\n",
    "    losses = []\n",
    "    n_iter = 0\n",
    "    total_loss = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    print_every = 50\n",
    "    sample_every = print_every * 5\n",
    "    \n",
    "    prev_states = get_initial_prev_states(\"train\")\n",
    "    output_holder = torch.zeros(timestamps-1, batch_size, 6*n_output_mixtures+1, device=device).float()\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for strokes, _, strokes_mask, one_hots, one_hot_lengths, one_hots_mask in train_dataloader:\n",
    "            strokes = strokes.to(device).float()\n",
    "            strokes_mask = strokes_mask.to(device).float()\n",
    "            one_hots = one_hots.to(device).float()\n",
    "            one_hots_mask = one_hots_mask.to(device).float()\n",
    "            one_hot_lengths = one_hot_lengths.to(device).float()\n",
    "            \n",
    "            batch_loss, ka = train_batch(\n",
    "                model,\n",
    "                optimizer,\n",
    "                strokes,\n",
    "                strokes_mask,\n",
    "                one_hots,\n",
    "                one_hot_lengths,\n",
    "                one_hots_mask,\n",
    "                prev_states,\n",
    "                output_holder)\n",
    "            \n",
    "            for k in prev_states:\n",
    "                prev_states[k] = detach(prev_states[k])\n",
    "                \n",
    "            output_holder = output_holder.detach()\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "            \n",
    "            if n_iter % print_every == 0:\n",
    "                avg_loss = total_loss/print_every\n",
    "                losses.append(avg_loss)\n",
    "                print(f\"iteration: {n_iter} \"\\\n",
    "                      f\"of {len(train_dataloader) * num_epochs}, \" \\\n",
    "                      f\"avg_loss: {avg_loss:.2f}, \"\\\n",
    "                      f\"timeSinceStart: {time.time() - start :.2f}, \"\\\n",
    "                      f\"Epoch: {epoch}\")\n",
    "                total_loss = 0\n",
    "                save_model(model, model_name)\n",
    "                \n",
    "            if n_iter % sample_every == 0:\n",
    "                    text = \"can you write this please\"\n",
    "                    sample_stroke, phis, hid1s, hid2s, alphas, betas, kappas = sample(model, text, bias=0.5)\n",
    "                    print(text)\n",
    "                    sample_stroke[1:, 0:2] *= torch.tensor(std, device=device)\n",
    "                    sample_stroke[1:, 0:2] += torch.tensor(means, device=device)\n",
    "                    draw(sample_stroke.cpu(), plot_end_points=False)\n",
    "                    print(\"actual kappa\")\n",
    "                    attention_plot(ka.squeeze().detach()[None])\n",
    "                    print(\"phi\")\n",
    "                    attention_plot(phis)\n",
    "                    print(\"hid_1\")\n",
    "                    attention_plot(hid1s)\n",
    "                    print(\"hid_2\")\n",
    "                    attention_plot(hid2s)\n",
    "                    print(\"alphas\")\n",
    "                    attention_plot(alphas[None])\n",
    "                    print(\"betas\")\n",
    "                    attention_plot(betas[None])\n",
    "                    print(\"kappas\")\n",
    "                    attention_plot(kappas[None])\n",
    "                \n",
    "            n_iter += 1\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_previous_state = True\n",
    "model = HandwritingSynthesis(input_size, hidden_size, n_output_mixtures, n_attn_mixtures, embedding_size).to(device)\n",
    "if load_previous_state:\n",
    "    checkpoint = torch.load(base_dir + model_name)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optim = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65eb320",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(model, optim, train_dataloader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddee1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf7149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
